{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate Keypoint Data from original and modified YOLOv7 pose-estimation model**"
      ],
      "metadata": {
        "id": "BEGWdOlSKAI_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRCENaeqQKL"
      },
      "source": [
        "# **1. Setting up Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SiG_AHyMYwj"
      },
      "source": [
        "# 1.1 Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrqovzPXy-Z8"
      },
      "outputs": [],
      "source": [
        "# mount google drive if necessary\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpqhrtjuMdOP"
      },
      "source": [
        "# 1.2 Cloning the repo and setting up dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewvc51Je8iot"
      },
      "outputs": [],
      "source": [
        "# execute only if you don't have the requirements\n",
        "%%bash\n",
        "cd /content/gdrive/MyDrive\n",
        "git clone https://github.com/WongKinYiu/yolov7.git\n",
        "cd yolov7\n",
        "# wget https://raw.githubusercontent.com/WongKinYiu/yolov7/u5/requirements.txt\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCZxHp_i7uhf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/yolov7') #making new directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtFXE96v-03G"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(\"/content/gdrive/MyDrive/yolov7/weights\"):\n",
        "  os.makedirs(\"/content/gdrive/MyDrive/yolov7/weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZbG3hlj_arW"
      },
      "source": [
        "# 1.3 Getting YOLOv7 Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A09g36a1_RmW"
      },
      "outputs": [],
      "source": [
        "# execute only if you don't have the weight files\n",
        "%%bash\n",
        "!wget -P /content/gdrive/MyDrive/yolov7/weights https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9gDg62ZMuS6"
      },
      "source": [
        "# 1.4 Helper code for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUd4EL67CQ3L"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages, letterbox\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, non_max_suppression_kpt, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
        "\n",
        "\n",
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2YOaDzZ9S2U"
      },
      "source": [
        "# **1.5 Configuration Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYNQUqYo9GGB"
      },
      "outputs": [],
      "source": [
        "classes_to_filter = [] #You can give list of classes to filter by name, Be happy you don't have to put class number. ['train','person' ]\n",
        "\n",
        "\n",
        "opt  = {\n",
        "\n",
        "    \"weights\": \"/content/gdrive/MyDrive/yolov7/weights/yolov7-w6-pose.pt\", # Path to weights file default weights are for nano model\n",
        "    \"yaml\"   : \"data/coco.yaml\",\n",
        "    \"img-size\": 640, # default image size\n",
        "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
        "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
        "    \"device\" : '0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
        "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTvryPT-Nhj"
      },
      "source": [
        "# **2. Inference on single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXyQTfal9yrh"
      },
      "outputs": [],
      "source": [
        "# Give path of source image\n",
        "source_image_path = '/content/gdrive/MyDrive/yolov7/inference/images/example.jpg' # provided an example image\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], opt['img-size']\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
        "\n",
        "  img0 = cv2.imread(source_image_path)\n",
        "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "  img = np.ascontiguousarray(img)\n",
        "  img = torch.from_numpy(img).to(device)\n",
        "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "  if img.ndimension() == 3:\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "  # Inference\n",
        "  t1 = time_synchronized()\n",
        "  pred = model(img, augment= False)[0]\n",
        "\n",
        "  # Apply NMS\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "\n",
        "      classes.append(names.index(class_name))\n",
        "\n",
        "  if classes:\n",
        "\n",
        "    classes = [i for i in range(len(names)) if i not in classes]\n",
        "\n",
        "\n",
        "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= [17], agnostic= False)\n",
        "  t2 = time_synchronized()\n",
        "  for i, det in enumerate(pred):\n",
        "    s = ''\n",
        "    s += '%gx%g ' % img.shape[2:]  # print string\n",
        "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "    if len(det):\n",
        "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "      for c in det[:, -1].unique():\n",
        "        n = (det[:, -1] == c).sum()  # detections per class\n",
        "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSl--Sn6A8XQ"
      },
      "outputs": [],
      "source": [
        "# plot the image\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq2qfQULJTAt"
      },
      "source": [
        "# **3. Inference on Video**\n",
        "\n",
        "**Note** Make sure to make relevant changes in arguments in argument section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai2CM4GCJZOS"
      },
      "source": [
        "# 3.1.1 Upload video from Local System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4845Io5SJSIj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY6AeZmYJaUa"
      },
      "source": [
        "# 3.1.2 Download video from Google Drive Link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3RVsOICCCtP"
      },
      "outputs": [],
      "source": [
        "#change URL\n",
        "!gdown --fuzzy https://drive.google.com/file/d/16voMoZOiP3Bm0W8Tpu5YotBBpxK_Fs-o/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD6w8__3Jfff"
      },
      "source": [
        "# 3.1.3 Download from any public URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y94Uf58GJhsy"
      },
      "outputs": [],
      "source": [
        "! wget PUBLIC_URL_TO_MP4/AVI_FILE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdWzdExC3zfg"
      },
      "source": [
        "# 3.2 Enter Video Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kps6gIQLJ5kE"
      },
      "outputs": [],
      "source": [
        "# give the full path to video, your video will be in the Yolov7 folder\n",
        "video_path = '/content/gdrive/MyDrive/yolov7/inference/videos/example.MOV' # provided an example video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMgJDFv3yqV"
      },
      "source": [
        "# 3.3 Run YOLOv7 inference on video\n",
        "## Modified for generating keypoint data and save as .csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WwmlroyJ_CF"
      },
      "outputs": [],
      "source": [
        "# Initializing video object\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "\n",
        "#Video information\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialzing object for writing video output\n",
        "output = cv2.VideoWriter('compressed_result.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))  # define the output video\n",
        "torch.cuda.empty_cache()\n",
        "# Initializing model and setting it for inference\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], opt['img-size']\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
        "\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "\n",
        "      classes.append(names.index(class_name))\n",
        "\n",
        "  if classes:\n",
        "\n",
        "    classes = [i for i in range(len(names)) if i not in classes]\n",
        "  print(names)\n",
        "  t0 = time_synchronized()\n",
        "  for j in range(nframes):\n",
        "      ret, img0 = video.read()\n",
        "\n",
        "      if ret:\n",
        "        img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "          img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "\n",
        "        pred = model(img, augment= False)[0]\n",
        "        # output, image => pred, img\n",
        "        t12 = time_synchronized() # t12 - t1 : propagation time per frame\n",
        "\n",
        "        pred = non_max_suppression_kpt(pred, opt['conf-thres'], opt['iou-thres'], # classes= classes, agnostic= False,\n",
        "                                  nc=model.yaml['nc'], # Number of Classes\n",
        "                                  nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
        "                                  kpt_label=True)\n",
        "        with torch.no_grad():\n",
        "          pred = output_to_keypoint(pred)\n",
        "\n",
        "        t2 = time_synchronized()  # t2 - t12 : NMS time per frame  # t2 - t1 : inference time per frame\n",
        "\n",
        "        nimg = img[0].permute(1, 2, 0) * 255\n",
        "        nimg = nimg.cpu().numpy().astype(np.uint8)\n",
        "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        pred_np = pred[0]\n",
        "        pred_df = pd.DataFrame(pred_np)\n",
        "        pred_df.to_csv(path_or_buf = f'/content/gdrive/MyDrive/pose_class/000.idle/000_{j}.csv', sep=' ')\n",
        "\n",
        "        for idx in range(pred.shape[0]):\n",
        "          plot_skeleton_kpts(nimg, pred[idx, 7:].T, 3)\n",
        "\n",
        "        nimg = cv2.resize(nimg, (w, h))\n",
        "        print(f\"propagation time: {t12-t1}\")\n",
        "        print(f\"NMS time: {t2-t12}\")\n",
        "        print(f\"inference time: {t2-t1}\")\n",
        "        print(f\"{j+1}/{nframes} frames processed \\n\")\n",
        "        output.write(nimg) # output.write(img0)\n",
        "      else:\n",
        "        break\n",
        "  t3 = time_synchronized()  # t3 - t0 : total time spent for vid pred\n",
        "\n",
        "print(f\"fps: {nframes/(t3-t0)}\")\n",
        "output.release()\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyTF6q95PO5m"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "# Input video path\n",
        "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
        "\n",
        "# Compressed video path\n",
        "compressed_path = \"/content/result_compressed.mp4\"\n",
        "\n",
        "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEqwSYIcapIQ"
      },
      "source": [
        "# 3.4 Download Inference Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg2JuyRrasa4"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
        "files.download(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS_IaWqBW77h"
      },
      "source": [
        "# **4. Inference on webcam**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h--RVkUvW_ty"
      },
      "source": [
        "# 4.1 Webcam Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCswC-EgW4Hs"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYvPdjQYXCzi"
      },
      "outputs": [],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], (512,640)\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz[0], imgsz[1]).to(device).type_as(next(model.parameters())))\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "\n",
        "      classes.append(names.index(class_name))\n",
        "\n",
        "  if classes:\n",
        "\n",
        "    classes = [i for i in range(len(names)) if i not in classes]\n",
        "\n",
        "  while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "      break\n",
        "\n",
        "    img0 = js_to_image(js_reply[\"img\"])\n",
        "    hpe_kpt_array = np.zeros([512,640,4], dtype=np.int8)\n",
        "    img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "    if img.ndimension() == 3:\n",
        "      img = img.unsqueeze(0)\n",
        "\n",
        "    # Inference\n",
        "    t1 = time_synchronized()\n",
        "    pred = model(img, augment= False)[0]\n",
        "    t12 = time_synchronized()\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression_kpt(pred, opt['conf-thres'], opt['iou-thres'], # classes= classes, agnostic= False,\n",
        "                                  nc=model.yaml['nc'], # Number of Classes\n",
        "                                  nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
        "                                  kpt_label=True)\n",
        "    with torch.no_grad():\n",
        "      pred = output_to_keypoint(pred)\n",
        "\n",
        "    t2 = time_synchronized()\n",
        "    nimg = img[0].permute(1, 2, 0) * 255\n",
        "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
        "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
        "    for idx, det in enumerate(pred):\n",
        "      # print(det, det.shape, type(det))\n",
        "      plot_skeleton_kpts(nimg, det[7:].T, 3)\n",
        "      # for *xyxy, conf, cls in reversed(det):\n",
        "        # plot_skeleton_kpts(xyxy, pred[idx, 7:].T, 3)\n",
        "    nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGBA)\n",
        "    hpe_kpt_array = cv2.resize(nimg, (640, 512))\n",
        "\n",
        "    # hpe_kpt_array[:,:,3] = (hpe_kpt_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_bytes = bbox_to_bytes(hpe_kpt_array)\n",
        "\n",
        "    bbox = bbox_bytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPd9UN9Kx8ol"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}